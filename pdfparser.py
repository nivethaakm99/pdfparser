# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UCVH71IgL0eFjtW2GbMCvy0TeiDMx_Kv
"""

import fitz # Still great for fast text extraction page by page
import re   # For powerful text pattern matching (like finding numbers)
import sys  # To interact with the system, like getting command-line arguments
import pandas as pd # To work with tabular data easily
import pdfplumber # Our new, friendlier table extraction tool!

# --- Configuration: Setting up our tools and rules ---

# These are the keywords we look for to understand if a number is in thousands, millions, etc.
# We're mapping them to their actual numerical scale factors.
SCALE_KEYWORDS = {
    'million': 1_000_000.0,
    'millions': 1_000_000.0,
    'k': 1_000.0,      # 'K' is a common shorthand for thousands
    'thousand': 1_000.0,
    'thousands': 1_000.0,
    'billion': 1_000_000_000.0,
    'billions': 1_000_000_000.0,
    'trillion': 1_000_000_000_000.0,
    'trillions': 1_000_000_000_000.0,
}

# This is a clever pattern to find almost any number in the text.
# It handles decimals, commas, currency symbols, and even negative numbers in parentheses.
NUMBER_REGEX = re.compile(
    r'\b'         # Ensures we match a whole word, not part of another word
    r'[-+]?'      # Optional: A plus or minus sign at the beginning
    r'[\(]?'      # Optional: An opening parenthesis (for negative numbers like (1,000))
    r'[\$€£]?'    # Optional: Currency symbols like $, €, £
    r'\s*'        # Optional: Any spaces after the currency symbol
    r'(\d{1,3}(?:[,\s]\d{3})*|\d+)'  # The main number: allows commas (e.g., 1,234,567) or just digits
    r'(?:\.\d+)?' # Optional: A decimal point followed by digits (e.g., .50)
    r'[\)]?'      # Optional: A closing parenthesis
    r'\b'         # Ensures we match a whole word at the end too
)

# --- Core Functions: Our PDF processing pipeline steps ---

def get_text_from_pdf(pdf_path):
    """
    Carefully opens a PDF and pulls out all its text, page by page using PyMuPDF.
    It's like reading the document from cover to cover.
    """
    page_texts = []
    try:
        document = fitz.open(pdf_path)
        print(f"\n  PyMuPDF: Found {document.page_count} pages to read for text extraction.")
        for page_num in range(document.page_count):
            page = document.load_page(page_num)
            page_texts.append(page.get_text("text"))
        document.close()
        print("  PyMuPDF: All text extracted successfully!")
    except Exception as e:
        print(f"Oops! Couldn't open or read the PDF at '{pdf_path}' with PyMuPDF. Here's why: {e}", file=sys.stderr)
        print("  Don't worry, we'll try to continue, but text analysis might be incomplete.", file=sys.stderr)
    return page_texts


def extract_tables_with_pdfplumber(pdf_path):
    """
    Extracts tables from a PDF document using pdfplumber.
    This is generally more reliable and easier to set up than Camelot.
    """
    all_extracted_tables_as_dfs = [] # We'll store tables as Pandas DataFrames
    try:
        print(f"\n  PDFPlumber: Rolling up sleeves to find tables in '{pdf_path}'...")
        with pdfplumber.open(pdf_path) as pdf:
            print(f"  PDFPlumber: Found {len(pdf.pages)} pages to scan for tables.")
            for page_num, page in enumerate(pdf.pages):
                tables_on_page = page.extract_tables({
                    "vertical_strategy": "lines",  # Look for vertical lines to define columns
                    "horizontal_strategy": "lines",# Look for horizontal lines to define rows
                    "snap_tolerance": 5,           # How close lines need to be to be considered snapped
                    "join_tolerance": 3,           # How close elements need to be to be joined
                    "edge_min_length": 3,          # Minimum length for detected lines
                    "min_words_vertical": 2,       # Minimum number of words in a vertical line to be considered a column
                    "min_words_horizontal": 2,     # Minimum number of words in a horizontal line to be considered a row
                })
                if tables_on_page:
                    print(f"  PDFPlumber: Found {len(tables_on_page)} table(s) on Page {page_num + 1}.")
                    for table_idx, table_data in enumerate(tables_on_page):
                        # pdfplumber extracts as list of lists, first sublist is header
                        if table_data:
                            # Convert to DataFrame for consistent processing later
                            # Use the first row as columns, rest as data
                            df = pd.DataFrame(table_data[1:], columns=table_data[0])
                            # Add page number as an attribute for consistency with how Camelot passed it
                            df.page = str(page_num + 1)
                            all_extracted_tables_as_dfs.append(df)
                # else: # No need for explicit "No tables found" per page
                #     print(f"  PDFPlumber: No tables found on Page {page_num + 1}.")
        print(f"  PDFPlumber: Finished scanning. Total tables found: {len(all_extracted_tables_as_dfs)}.")

    except Exception as e:
        print(f"Uh oh! PDFPlumber ran into an issue extracting tables from '{pdf_path}'. Error: {e}", file=sys.stderr)
        print("  Table analysis might be incomplete.", file=sys.stderr)
    return all_extracted_tables_as_dfs


def clean_and_convert_number(raw_num_string):
    """
    Takes a messy number string (like '$1,234.56') and cleans it up
    to turn it into a proper floating-point number.
    Handles negative numbers in parentheses too!
    """
    # Remove commas, currency symbols, and any extra spaces
    cleaned_string = raw_num_string.replace(',', '').replace('$', '').replace('€', '').replace('£', '').strip()

    # If it's a negative number like (123.45), convert it correctly
    if cleaned_string.startswith('(') and cleaned_string.endswith(')'):
        try:
            return -float(cleaned_string[1:-1]) # Remove parentheses and negate
        except ValueError:
            return None # Couldn't convert, so return nothing
    else:
        try:
            return float(cleaned_string) # Just convert directly
        except ValueError:
            return None # Couldn't convert, so return nothing

def infer_scale_from_context(text_snippet, number_location_span=None, proximity_radius=20):
    """
    This function is a detective for numbers! It looks for words like 'million' or 'thousands'
    around a number to figure out its true scale.
    It prioritizes words found very close to the number.
    """
    text_snippet_lower = text_snippet.lower()

    # First, let's check for direct keywords right next to our number
    for keyword, scale in SCALE_KEYWORDS.items():
        # Create a pattern to find the whole word (e.g., '\bmillion\b')
        keyword_pattern = r'\b' + re.escape(keyword) + r'\b'

        for match_object in re.finditer(keyword_pattern, text_snippet_lower):
            if number_location_span: # If we know where the number is, check proximity
                keyword_start, keyword_end = match_object.span()
                # Is the keyword within our 'proximity_radius' of the number?
                if (keyword_end <= number_location_span[0] and (number_location_span[0] - keyword_end) < proximity_radius) or \
                   (keyword_start >= number_location_span[1] and (keyword_start - number_location_span[1]) < proximity_radius):
                    return scale, keyword # Found a strong, local scale!

            else: # If we don't have a specific number location, any keyword in the snippet counts
                return scale, keyword # Found a general scale in this context

    return 1.0, None # If no scale keyword is found, assume no scaling needed (factor of 1)

def find_and_apply_scales(all_page_texts, all_tables_found):
    """
    This is where the magic happens! We go through every number we find,
    whether it's in a table or just in the text, and try to apply the correct
    scale (thousands, millions, etc.) based on its surroundings.
    """
    all_scaled_numbers_and_pages = []

    # First, let's try to get a 'default' scale for the entire document.
    # We'll check the first few pages for common disclaimers like "Dollars in Thousands".
    # This acts as a fallback if no specific scale is found closer to a number.
    document_wide_context = "".join(all_page_texts[:min(len(all_page_texts), 5)]).lower()
    document_default_scale, document_default_keyword = infer_scale_from_context(document_wide_context)
    print(f"\n--- Document-Wide Default Scale (Our Best Guess Fallback) ---")
    print(f"  Default Scale: {document_default_scale:,.0f} (Keyword: '{document_default_keyword if document_default_keyword else 'None Found'}')")
    print(f"  Context scanned (first 200 chars): '{document_wide_context[:200]}...'")


    # We iterate through all page texts and extract tables separately.
    # This design allows us to have a unified 'all_tables_found' list
    # even though tables might not be explicitly associated with page_texts
    # in the loop for 'current_page_number'.
    # We'll rely on the 'page' attribute we manually added to the DataFrame.

    for page_index, page_content in enumerate(all_page_texts):
        current_page_number = page_index + 1
        print(f"\n--- Now Processing Page {current_page_number} ---")

        page_content_lower = page_content.lower()
        # Let's also try to get a general scale for this specific page.
        page_general_scale, page_general_keyword = infer_scale_from_context(page_content_lower)
        print(f"  Page-Level General Scale (Overall Page Hint): {page_general_scale:,.0f} (Keyword: '{page_general_keyword if page_general_keyword else 'None Found'}')")


        # --- Step 1: Process numbers found within tables for the current page ---
        # Filter tables that belong to the current page
        tables_on_this_page = [df for df in all_tables_found if hasattr(df, 'page') and df.page == str(current_page_number)]

        if tables_on_this_page:
            for table_idx, table_df in enumerate(tables_on_this_page):
                if table_df.empty:
                    continue

                print(f"    Analyzing Table {table_idx+1} (Rows: {table_df.shape[0]}, Cols: {table_df.shape[1]}) on Page {current_page_number}.")

                # Try to find scales in the table headers (e.g., 'Cost (in thousands)')
                column_specific_scales = [1.0] * len(table_df.columns)
                column_specific_keywords = [None] * len(table_df.columns)

                # Ensure columns are strings to prevent errors if they are not
                header_row_values = [str(col) for col in table_df.columns.tolist()] if not table_df.empty else []
                for col_index, header_cell_text in enumerate(header_row_values):
                    header_scale, header_keyword = infer_scale_from_context(header_cell_text, number_location_span=None)
                    if header_scale != 1.0:
                        column_specific_scales[col_index] = header_scale
                        column_specific_keywords[col_index] = header_keyword
                        print(f"      Header for Column '{header_cell_text}' suggests scale: {header_scale:,.0f} (Keyword: '{header_keyword}')")

                # Now, go through each cell in the table data
                for row_index in range(table_df.shape[0]):
                    for col_index in range(table_df.shape[1]):
                        cell_content = table_df.iloc[row_index, col_index]
                        cell_text = str(cell_content).strip()

                        # Find all numbers within this cell
                        for number_match in NUMBER_REGEX.finditer(cell_text):
                            raw_number_string = number_match.group(0)
                            cleaned_number = clean_and_convert_number(raw_number_string)

                            if cleaned_number is not None:
                                # Determine the most effective scale for this number
                                current_effective_scale = 1.0
                                current_effective_keyword = None

                                # 1. Prioritize column-specific scale from header
                                if column_specific_scales[col_index] != 1.0:
                                    current_effective_scale = column_specific_scales[col_index]
                                    current_effective_keyword = column_specific_keywords[col_index]
                                # 2. Fallback to page-level general scale
                                elif page_general_scale != 1.0:
                                    current_effective_scale = page_general_scale
                                    current_effective_keyword = page_general_keyword
                                # 3. Fallback to document-wide default scale
                                elif document_default_scale != 1.0:
                                    current_effective_scale = document_default_scale
                                    current_effective_keyword = document_default_keyword
                                # Otherwise, stick with 1.0 (no scale)

                                final_scaled_number = cleaned_number * current_effective_scale
                                all_scaled_numbers_and_pages.append((final_scaled_number, current_page_number))

                                print(f"      [Table Number] Raw: '{raw_number_string}', Cleaned: {cleaned_number}, Scaled: {final_scaled_number:,.2f} "
                                      f"(Scale: {current_effective_scale:,.0f}, Keyword: '{current_effective_keyword if current_effective_keyword else 'None'}') "
                                      f"| Context: '{cell_text}' (Page: {current_page_number}, Table {table_idx+1})")
        else:
            print(f"  PDFPlumber: No tables detected on Page {current_page_number}.")

        # --- Step 2: Process numbers found in general text (outside of tables) ---
        print(f"  Now scanning general text on Page {current_page_number} for numbers...")
        # Split text into lines to give more localized context for each number
        text_lines = page_content.split('\n')
        for line_index, line_text in enumerate(text_lines):
            for number_match in NUMBER_REGEX.finditer(line_text):
                raw_number_string = number_match.group(0)
                cleaned_number = clean_and_convert_number(raw_number_string)
                number_span = number_match.span() # Where exactly the number was found in the line

                if cleaned_number is not None:
                    final_effective_scale = 1.0
                    final_effective_keyword = None

                    # 1. First, check the immediate surroundings on the same line
                    local_scale, local_keyword = infer_scale_from_context(line_text, number_span)

                    if local_scale != 1.0:
                        # If we find a specific scale right next to the number, that's our best bet!
                        final_effective_scale = local_scale
                        final_effective_keyword = local_keyword
                    else:
                        # If no immediate scale, check the broader page context around the number
                        page_proximate_scale, page_proximate_keyword = \
                            infer_scale_from_context(page_content_lower, number_span)

                        if page_proximate_scale != 1.0:
                            final_effective_scale = page_proximate_scale
                            final_effective_keyword = page_proximate_keyword
                        else:
                            # As a last resort, use the document-wide default scale
                            if document_default_scale != 1.0:
                                final_effective_scale = document_default_scale
                                final_effective_keyword = document_default_scale_keyword
                            # Otherwise, the scale remains 1.0 (no scaling)

                    final_scaled_number = cleaned_number * final_effective_scale
                    all_scaled_numbers_and_pages.append((final_scaled_number, current_page_number))

                    print(f"    [Text Number] Raw: '{raw_number_string}', Cleaned: {cleaned_number}, Scaled: {final_scaled_number:,.2f} "
                          f"(Scale: {final_effective_scale:,.0f}, Keyword: '{final_effective_keyword if final_effective_keyword else 'None'}') "
                          f"| Context: '{line_text.strip()}' (Page: {current_page_number}, Line: {line_index+1})")

    return all_scaled_numbers_and_pages


# --- Main Execution: Putting it all together ---

def main():
    """
    This is the main brain of our script. It orchestrates all the steps
    to parse the PDF, find numbers, apply scales, and report the biggest one!
    """
    # Check if a PDF path was provided as a command-line argument
    if len(sys.argv) < 2:
        print("Hey there! To run this, please tell me which PDF document to analyze.", file=sys.stderr)
        print("Usage: python your_script_name.py <path_to_pdf_document>", file=sys.stderr)
        sys.exit(1)

    # Get the PDF path from the command-line arguments
    pdf_path = sys.argv[1]

    print(f"\n--- Let's get started! Processing PDF: '{pdf_path}' ---\n")

    # --- Step 1: Extracting Tables with PDFPlumber ---
    print(">> Step 1/3: Gathering tables from the PDF using PDFPlumber (this might take a moment)...")
    tables_from_pdf = extract_tables_with_pdfplumber(pdf_path)

    # --- Step 2: Extracting All Text with PyMuPDF ---
    print("\n>> Step 2/3: Pulling out all the plain text from each page using PyMuPDF...")
    all_pdf_texts = get_text_from_pdf(pdf_path)
    if not all_pdf_texts and not tables_from_pdf:
        print("Hmm, couldn't find any text OR tables in the PDF. There's nothing to analyze! Exiting.", file=sys.stderr)
        sys.exit(1)

    # --- Step 3: Finding Numbers and Applying Scales ---
    print("\n>> Step 3/3: Now, let's find all the numbers and figure out their true values (detailed output below)...")
    scaled_numbers_info = find_and_apply_scales(all_pdf_texts, all_tables_found=tables_from_pdf)

    # --- Final Report ---
    if scaled_numbers_info:
        # Find the biggest number we processed
        largest_number_details = max(scaled_numbers_info, key=lambda x: x[0])
        largest_value = largest_number_details[0]
        page_where_found = largest_number_details[1]

        print("\n" + "="*50)
        print("🎉 ANALYSIS COMPLETE! 🎉")
        print(f"The BIGGEST numerical value we found (after careful scaling) is: {largest_value:,.2f}")
        print(f"You can find this number on Page: {page_where_found}")
        print("="*50 + "\n")
    else:
        print("\n😔 Couldn't find any numerical values in the document to analyze. Maybe try a different PDF?", file=sys.stderr)

if __name__ == "__main__":
    main()

